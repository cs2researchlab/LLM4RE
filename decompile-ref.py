from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Use the V2 model path (update if your model path differs)
model_path = 'LLM4Binary/llm4decompile-1.3b-v2'  # V2 Model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)

# Assume you have already generated a Ghidra decompiled pseudo-code file.
fileName = '/home/logan/Documents/main'
input_file = f"{fileName}.c"  # Ghidra output file
output_file = f"{fileName}_refined.c"  # Output file

# Read the pseudo-code prompt generated by Ghidra
with open(input_file, 'r', encoding='utf-8') as f:
    ghidra_prompt = f.read()

# Tokenize and generate decompiled C code
inputs = tokenizer(ghidra_prompt, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=2048)

c_func_decompile = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):-1])

# Save the decompiled function to a file
with open(output_file, 'w', encoding='utf-8') as f:
    f.write(c_func_decompile)

print(f'Refined function saved to: {output_file}')

